{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise with Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excerpt of what I've been trying to do with a large amount of weather data that is not stored in an intuitive format. <br> \n",
    "The main goals of sorting through the dataset are: <br>\n",
    "\n",
    "1. Read in all the files, which include several weather stations sites and years.\n",
    "2. Pick out hourly temperature and precipitation from raw data.\n",
    "3. Store temperature and precipitation information in separate dataframe.\n",
    "4. Identify gap size in temperature & precipitation data.\n",
    "5. Select site-years that have acceptable gaps that have both temperature and precipitaiton data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more info on the weather dataset:\n",
    "- The data is too big to put on github! Please find a subset of the data [here](https://drive.google.com/drive/folders/1IVfKxV3qbp89P2tBlwKqLVVQeClBA8rt?usp=sharing).\n",
    "- NOAA maintains this dataset in the Integrated Surface Hourly (ISH) database\n",
    "- Raw data can be downloaded [here](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/) if anyone might be interested.\n",
    "- The data is recorded hourly at weather stations across the US, and the information is stored in a special format called the ASCII character format.\n",
    "- Each data file records information for a particular weather station for the whole year, and each line of text within the data includes lots of information recorded on a hourly timestep, so the file will include 24*365 = 8760 lines of data if not a leap year. <br>\n",
    "There are three parts within the data that have relevant information to extract (refer to \"ISH_Manual.pdf\" included in folder:\n",
    "\n",
    "> 1. The control data section (p.4): <br>\n",
    "In here you can find timestamps, and weather station info\n",
    "> 2. The mandatory data section (p.7 & p.9): <br>\n",
    "You will find temperature data here\n",
    "> 3. The additional data section (p.12): <br>\n",
    "You will find precipitation data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to think about:\n",
    "\n",
    "- This is code I put together when I just started transitioning into python (and still am!). I imagnie it has room for lots of improvement. <br> Feel free to work with it and see what other ideas you come up with, but also feel free to start from scratch and see what you come up with!\n",
    "- What are some good practices when wrangling data?\n",
    "- The code is slow : / <br> In reality, I have data from ~200-273 sites to process from year 1961-1990. Are there ways to make this process faster?\n",
    "- Data management?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in individual weather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n"
     ]
    }
   ],
   "source": [
    "# Note: this code will take a couple minutes to finish running\n",
    "\n",
    "years = np.arange(1961,1966) # years of data \n",
    "df_temp_all = pd.DataFrame() # setting up an empty dataframe to store final info\n",
    "df_precip_all = pd.DataFrame() # same for precip\n",
    "\n",
    "\n",
    "for i in years:\n",
    "    file_list = glob.glob(\"./data/\" + str(i) + \"/*\") # make sure to update this to wherever you put your data.\n",
    "                                                     # refer to the glob tutorial EL put togehter for more info on glob: \n",
    "                                                     # \"exercise_geo_plotting/TUTORIAL_glob.ipynb\n",
    "    timepoints = pd.date_range(start = str(i) + \"-01-01\", \n",
    "                               end = str(i) + \"-12-31 23:00:00\", freq = \"H\")\n",
    "    df_temp = pd.DataFrame(index=timepoints)\n",
    "    df_precip = pd.DataFrame(index=timepoints)\n",
    "    \n",
    "    print(i) # just so you know where the process is at\n",
    "\n",
    "    for j in file_list:\n",
    "        WBAN_id = j.split(\"/\")[-1][7:12] # weather station site ID\n",
    "        df = pd.read_csv(j, sep='\\t', header=None, squeeze=True)        \n",
    "        lines = np.arange(df.shape[0])   \n",
    "        time = list()\n",
    "        temp = list()\n",
    "        precip = list()\n",
    "        \n",
    "        for k in lines:\n",
    "            timestamp = pd.to_datetime(df[k][15:27], format=\"%Y%m%d%H%M\")\n",
    "            time.append(timestamp)\n",
    "            \n",
    "            if int(df[k][87:92]) == 9999:\n",
    "                temp.append(\"NaN\")\n",
    "            else:\n",
    "                temp.append(int(df[k][87:92])/10)\n",
    "                \n",
    "            if precip_pos == -1:\n",
    "                precip.append(\"NaN\")\n",
    "            elif int(df[k][precip_pos+5:precip_pos+9]) == 9999:\n",
    "                precip.append(\"NaN\")\n",
    "            else:\n",
    "                precip.append(int(df[k][precip_pos+5:precip_pos+9]))\n",
    "            \n",
    "                                \n",
    "        df_t = pd.DataFrame({WBAN_id: temp}, index=time)\n",
    "        df_t = df_t[~df_t.index.duplicated()] # some sites have data records on sub-hourly time scale\n",
    "                                              # but I only want hourly data\n",
    "        df_temp = pd.concat([df_temp, df_t], axis= 1, \n",
    "                            join_axes=[df_temp.index], sort=False)\n",
    "            \n",
    "        df_p = pd.DataFrame({WBAN_id:precip}, index=time)\n",
    "        df_p = df_p[~df_p.index.duplicated()]\n",
    "        df_precip = pd.concat([df_precip, df_p], axis= 1, \n",
    "                              join_axes=[df_precip.index], sort=False)\n",
    "            \n",
    "            \n",
    "\n",
    "    frames_temp = [df_temp_all, df_temp]\n",
    "    df_temp_all = pd.concat(frames_temp, sort=False)\n",
    "    \n",
    "    frames_precip = [df_precip_all, df_precip]\n",
    "    df_precip_all = pd.concat(frames_precip, sort=False)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determine data gap size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Loop\n",
    "# looking through the temp & precip dataset to pick out usable site-years\n",
    "\n",
    "datasets = list([df_temp_all, df_precip_all]) # weather datasets to process\n",
    "finalist = list([[], []]) # order: [0]-temp, [1]-precip\n",
    "years = np.arange(1961, 1966)\n",
    "growseason_start = \"-05-01 00:00:00\" # defining a growing season\n",
    "growseason_end = \"-10-31 23:00:00\"\n",
    "crit_hrs = 3 # critical hrs of missing data\n",
    "\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    data = datasets[i]\n",
    "    siteyears_all = list()\n",
    "    sites = data.columns\n",
    "    \n",
    "    for j in years:\n",
    "        start_time = str(j) + growseason_start\n",
    "        end_time = str(j) + growseason_end\n",
    "        siteyears = list()\n",
    "        \n",
    "        for k in sites:\n",
    "            df = data.loc[start_time:end_time, k]\n",
    "            df = pd.DataFrame(df)\n",
    "            df[\"group\"] = df.notnull().astype(int)\n",
    "            df[\"group\"] = df.group.cumsum()\n",
    "            df = df[df.iloc[:,0].isnull()]\n",
    "            df[\"count\"] = df.groupby(\"group\")[\"group\"].transform(\"size\")\n",
    "            df.drop_duplicates(\"group\")\n",
    "            \n",
    "            if df[df[\"count\"] > crit_hrs].shape[0] == 0:\n",
    "                use_siteyear = str(j) + \"_\" + str(k)\n",
    "                siteyears.append(use_siteyear)\n",
    "        \n",
    "        siteyears_all.extend(siteyears)\n",
    "    \n",
    "    finalist[i] = siteyears_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Determine overlapping site-years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "238\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "yearsites_temp = finalist[0]\n",
    "yearsites_precip = finalist[1]\n",
    "yearsites = list(set(yearsites_temp) & set(yearsites_precip))\n",
    "yearsites.sort()\n",
    "\n",
    "print(len(yearsites_temp))\n",
    "print(len(yearsites_precip))\n",
    "print(len(yearsites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1961_03822',\n",
       " '1961_03940',\n",
       " '1961_12924',\n",
       " '1961_13722',\n",
       " '1961_13723',\n",
       " '1961_13880',\n",
       " '1961_13889',\n",
       " '1961_13959',\n",
       " '1961_13962',\n",
       " '1961_13964']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearsites[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
